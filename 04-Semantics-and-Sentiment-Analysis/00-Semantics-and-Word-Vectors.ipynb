{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9becc41-dd67-45b8-a3f7-9c72d285c976",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f02abb4a-83c8-4d50-b70d-6d880ccb68cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Semantics and Word Vectors\n",
    "Sometimes called \"opinion mining\", [Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis) defines ***sentiment analysis*** as\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">\"the use of natural language processing ... to systematically identify, extract, quantify, and study affective states and subjective information.<br>\n",
    "Generally speaking, sentiment analysis aims to determine the attitude of a speaker, writer, or other subject with respect to some topic or the overall contextual polarity or emotional reaction to a document, interaction, or event.\"</div>\n",
    "\n",
    "Up to now we've used the occurrence of specific words and word patterns to perform test classifications. In this section we'll take machine learning even further, and try to extract intended meanings from complex phrases. Some simple examples include:\n",
    "* Python is relatively easy to learn.\n",
    "* That was the worst movie I've ever seen.\n",
    "\n",
    "However, things get harder with phrases like:\n",
    "* I do not dislike green eggs and ham. (requires negation handling)\n",
    "\n",
    "The way this is done is through complex machine learning algorithms like [word2vec](https://en.wikipedia.org/wiki/Word2vec). The idea is to create numerical arrays, or *word embeddings* for every word in a large corpus. Each word is assigned its own vector in such a way that words that frequently appear together in the same context are given vectors that are close together. The result is a model that may not know that a \"lion\" is an animal, but does know that \"lion\" is closer in context to \"cat\" than \"dandelion\".\n",
    "\n",
    "It is important to note that *building* useful models takes a long time - hours or days to train a large corpus - and that for our purposes it is best to import an existing model rather than take the time to train our own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f61279fe-bbcd-44af-ab98-3533c808e9f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "___\n",
    "# Installing Larger spaCy Models\n",
    "Up to now we've been using spaCy's smallest English language model, [**en_core_web_sm**](https://spacy.io/models/en#en_core_web_sm) (35MB), which provides vocabulary, syntax, and entities, but not vectors. To take advantage of built-in word vectors we'll need a larger library. We have a few options:\n",
    "> [**en_core_web_md**](https://spacy.io/models/en#en_core_web_md) (116MB) Vectors: 685k keys, 20k unique vectors (300 dimensions)\n",
    "> <br>or<br>\n",
    "> [**en_core_web_lg**](https://spacy.io/models/en#en_core_web_lg) (812MB) Vectors: 685k keys, 685k unique vectors (300 dimensions)\n",
    "\n",
    "If you plan to rely heavily on word vectors, consider using spaCy's largest vector library containing over one million unique vectors:\n",
    "> [**en_vectors_web_lg**](https://spacy.io/models/en#en_vectors_web_lg) (631MB) Vectors: 1.1m keys, 1.1m unique vectors (300 dimensions)\n",
    "\n",
    "For our purposes **en_core_web_md** should suffice.\n",
    "\n",
    "### From the command line (you must run this as admin or use sudo):\n",
    "\n",
    "> `activate spacyenv`&emsp;*if using a virtual environment*   \n",
    "> \n",
    "> `python -m spacy download en_core_web_md`  \n",
    "> `python -m spacy download en_core_web_lg`&emsp;&emsp;&ensp;*optional library*  \n",
    "> `python -m spacy download en_vectors_web_lg`&emsp;*optional library*  \n",
    "\n",
    "> ### If successful, you should see a message like: \n",
    "> <tt><br>\n",
    "> **Linking successful**<br>\n",
    "> C:\\Anaconda3\\envs\\spacyenv\\lib\\site-packages\\en_core_web_md --><br>\n",
    "> C:\\Anaconda3\\envs\\spacyenv\\lib\\site-packages\\spacy\\data\\en_core_web_md<br>\n",
    "> <br>\n",
    "> You can now load the model via spacy.load('en_core_web_md')</tt>\n",
    "\n",
    "<font color=green>Of course, we have a third option, and that is to train our own vectors from a large corpus of documents. Unfortunately this would take a prohibitively large amount of time and processing power.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d40990e-f2d5-40bf-800e-021e6d783e20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (58.0.4)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /databricks/python3/lib/python3.9/site-packages (from spacy) (1.20.3)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /databricks/python3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: typing-extensions, catalogue, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, tqdm, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.6 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 tqdm-4.65.0 typer-0.7.0 typing-extensions-4.5.0 wasabi-1.1.1\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e70e1f1-4ded-4212-86d0-acdf06791bb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (58.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (21.0)\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /databricks/python3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.20.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f3a1e3e2-10d7-4ecf-ba6e-c3ef64c919e0/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /databricks/python3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.5.0\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install \"https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bea6256-e3f9-4a5e-9bab-eeb164fb349f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "___\n",
    "# Word Vectors\n",
    "Word vectors - also called *word embeddings* - are mathematical descriptions of individual words such that words that appear frequently together in the language will have similar values. In this way we can mathematically derive *context*. As mentioned above, the word vector for \"lion\" will be closer in value to \"cat\" than to \"dandelion\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a29829-e0f9-468a-8af6-38eef43be2c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Vector values\n",
    "So what does a word vector look like? Since spaCy employs 300 dimensions, word vectors are stored as 300-item arrays.\n",
    "\n",
    "Note that we would see the same set of values with **en_core_web_md** and **en_core_web_lg**, as both were trained using the [word2vec](https://en.wikipedia.org/wiki/Word2vec) family of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9436b1-a07c-4ad3-8e02-6bb46bd25c58",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')  # make sure to use a larger model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539232d9-ceb3-44a6-8088-d58b926b3aa1",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: array([ 3.7032e+00,  4.1982e+00, -5.0002e+00, -1.1322e+01,  3.1702e-02,\n",
      "       -1.0255e+00, -3.0870e+00, -3.7327e+00,  5.3875e-01,  3.5679e+00,\n",
      "        6.9276e+00,  1.5793e+00,  5.1188e-01,  3.1868e+00,  6.1534e+00,\n",
      "       -4.8941e+00, -2.9959e-01, -3.6276e+00,  2.3825e+00, -1.4402e+00,\n",
      "       -4.7577e+00,  4.3607e+00, -4.9814e+00, -3.6672e+00, -1.8052e+00,\n",
      "       -2.1888e+00, -4.2875e+00,  5.5712e+00, -5.2875e+00, -1.8346e+00,\n",
      "       -2.2015e+00, -7.7091e-01, -4.8260e+00,  1.2464e+00, -1.7945e+00,\n",
      "       -8.1280e+00,  1.9994e+00,  1.1413e+00,  3.8032e+00, -2.8783e+00,\n",
      "       -4.2136e-01, -4.4177e+00,  7.7456e+00,  4.9535e+00,  1.7402e+00,\n",
      "        1.8275e-01,  2.4218e+00, -3.1496e+00, -3.8057e-02, -2.9818e+00,\n",
      "        8.3396e-01,  1.1531e+01,  3.5684e+00,  2.5970e+00, -2.8438e+00,\n",
      "        3.2755e+00,  4.5674e+00,  3.2219e+00,  3.4206e+00,  1.1200e-01,\n",
      "        1.0303e-01, -5.8396e+00,  4.6370e-01,  2.7750e+00, -5.3713e+00,\n",
      "       -5.0247e+00, -2.0212e+00,  5.8772e-01,  1.1569e+00,  1.3224e+00,\n",
      "        4.3994e+00,  2.0444e+00,  2.1343e+00, -1.9023e+00,  2.1469e+00,\n",
      "       -2.9085e+00,  4.8429e-01, -3.3544e-01,  1.4484e+00, -1.5770e+00,\n",
      "       -1.1307e+00,  2.8320e+00,  6.2041e-01,  3.7994e+00, -3.1162e-01,\n",
      "       -6.9221e+00,  7.1342e+00,  7.2441e+00, -8.9326e+00, -2.7927e+00,\n",
      "        2.6613e-01,  6.7547e-01,  6.7293e+00, -5.8127e+00,  3.1567e+00,\n",
      "       -1.0634e+00, -1.5733e+00,  1.3534e+00,  3.9218e-01, -8.7077e+00,\n",
      "        3.4229e-02,  3.3251e+00,  4.6713e+00,  1.1865e-02,  9.8345e-01,\n",
      "       -5.3206e-02, -9.1613e+00,  6.0161e+00, -2.2223e+00,  2.5015e+00,\n",
      "       -6.0702e-01, -3.6344e-02,  7.1884e+00, -1.4431e+00,  2.6156e+00,\n",
      "       -1.0148e+00,  4.1225e+00, -1.8472e+00,  4.6292e+00, -2.6506e+00,\n",
      "       -1.8937e+00,  4.1749e+00, -9.6644e+00, -2.4813e+00, -2.7637e+00,\n",
      "       -1.0624e+00,  3.5988e+00,  4.9833e+00,  6.4499e-01,  2.5784e-01,\n",
      "        9.8727e-01, -4.2485e+00,  3.4272e-01, -2.2270e+00, -1.8957e+00,\n",
      "        8.0796e-01, -2.0265e+00, -6.1828e+00, -2.2378e+00,  2.8216e+00,\n",
      "       -2.0050e+00, -3.8924e+00, -2.9364e-01, -1.6128e+00, -6.7874e-01,\n",
      "       -1.9855e+00,  1.8221e-01,  2.1575e+00,  4.9825e-01, -1.7326e+00,\n",
      "        4.7886e+00,  2.9904e+00,  8.3447e-01, -4.7417e+00,  2.4697e+00,\n",
      "        1.3751e+00,  4.5358e+00,  6.5386e-01,  5.5413e+00,  2.3963e+00,\n",
      "        1.0031e+00, -8.0664e-01, -1.4126e+00,  2.8689e+00, -8.7339e+00,\n",
      "       -2.7457e+00, -3.1805e-01, -2.4484e-01,  3.7117e+00, -1.8636e+00,\n",
      "        2.9959e-01,  6.5062e-02, -1.5682e+00,  1.5876e+00,  6.9224e-01,\n",
      "       -6.7734e+00,  3.1065e+00,  2.3973e+00, -3.5138e+00,  3.4460e+00,\n",
      "        3.4252e+00, -5.1906e+00, -6.9372e-01,  1.9435e+00, -1.5669e-01,\n",
      "        1.9710e+00,  8.7743e-01, -8.3110e+00, -4.0306e-01, -5.0165e+00,\n",
      "       -5.6309e-02,  4.9249e+00, -7.1053e+00, -5.2338e+00,  2.3535e+00,\n",
      "       -2.5255e+00, -2.7785e+00,  5.0149e+00, -2.8405e+00, -1.8614e+00,\n",
      "        2.8818e-03,  1.3281e+00,  1.0194e+00,  3.5155e+00,  2.7971e-01,\n",
      "        1.3251e+00,  1.4386e+00, -6.1719e-01, -2.6864e+00, -3.9613e+00,\n",
      "        4.5749e+00, -1.0939e+00,  1.3289e+00, -9.5484e-01, -5.4675e+00,\n",
      "        2.1607e+00,  5.0715e-01,  1.4860e-01, -4.8571e+00, -2.2213e+00,\n",
      "       -2.3498e-01, -4.2629e+00, -8.7002e-01,  3.3796e+00, -4.3989e+00,\n",
      "        6.1047e+00,  3.7927e+00, -6.0760e+00,  3.1840e+00, -8.3104e-01,\n",
      "       -5.4015e+00, -6.2916e+00,  1.2497e+00,  1.8026e+00, -3.4535e+00,\n",
      "       -2.1652e-01, -1.4958e+00,  5.7946e-01,  2.2505e+00,  2.0868e+00,\n",
      "        3.9621e-01,  1.6076e+00,  4.0635e+00, -3.4088e+00, -1.0590e+00,\n",
      "       -3.6376e+00,  2.0501e+00,  1.4785e+00, -1.8906e+00, -2.6215e-01,\n",
      "       -5.1386e+00,  3.7029e+00, -1.8151e+00, -3.2759e+00, -5.1866e+00,\n",
      "        2.5485e-01, -4.5696e+00,  1.0147e+01, -3.0195e+00, -2.4640e+00,\n",
      "        7.5459e-01, -5.6395e+00, -5.4095e+00, -2.4363e+00, -4.3922e-01,\n",
      "       -4.0911e+00, -3.5194e+00,  1.8031e+00, -1.3644e-01,  6.7990e+00,\n",
      "        5.8461e+00,  5.3452e-01,  1.1042e+00,  3.5698e+00,  4.4668e+00,\n",
      "       -2.4537e+00, -2.1832e+00,  1.5293e+00, -1.9414e+00, -8.8675e-02,\n",
      "       -1.1825e+00, -3.9996e+00,  2.8077e+00, -1.8000e+00,  4.2545e+00,\n",
      "       -1.3813e+00, -2.2921e+00,  3.7889e+00, -1.5837e+00, -7.2078e-01,\n",
      "        4.7743e+00, -3.0923e+00,  8.4709e+00,  3.0132e-01, -5.6173e+00,\n",
      "       -5.4610e-01, -4.8459e+00,  6.0303e+00, -6.9664e+00,  3.1445e+00],\n",
      "      dtype=float32)"
     ]
    }
   ],
   "source": [
    "nlp(u'cat').vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc49f831-e7f7-43fb-a8ae-463e46b6d60e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What's interesting is that Doc and Span objects themselves have vectors, derived from the averages of individual token vectors. <br>This makes it possible to compare similarities between whole documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550f9458-97e6-4769-88fa-c736a9f5089b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: array([-1.69569147e+00,  2.14644998e-01, -9.03023899e-01,  1.86704069e-01,\n",
      "        3.63233805e+00, -1.86782926e-01,  2.43209034e-01,  3.60478401e+00,\n",
      "        1.14123464e+00, -5.80472648e-01,  3.81675982e+00,  1.62608492e+00,\n",
      "       -2.85950589e+00,  2.53757060e-01,  1.46926808e+00,  5.53511977e-01,\n",
      "        7.59729862e-01, -3.43407005e-01,  1.11228299e+00, -2.24132299e+00,\n",
      "       -4.01161134e-01, -4.10584033e-01,  3.20387989e-01, -2.08018613e+00,\n",
      "        1.20925140e+00, -5.05361974e-01, -3.68701220e+00, -2.67094684e+00,\n",
      "        1.49126089e+00,  2.22644877e+00, -1.02197933e+00, -6.48231983e-01,\n",
      "       -9.67403054e-01, -1.81315970e+00, -2.92256403e+00, -2.27995801e+00,\n",
      "       -6.06574953e-01,  1.40613389e+00,  2.20047617e+00, -9.33881775e-02,\n",
      "        1.20177758e+00,  2.25381583e-01,  6.34011030e-01, -4.98126030e-01,\n",
      "       -1.14107490e+00, -2.31033593e-01, -2.48855948e-01, -1.17482305e+00,\n",
      "       -2.28650999e+00,  4.14370060e-01,  1.42937988e-01,  3.22212291e+00,\n",
      "       -8.49860534e-02, -1.59288502e+00,  4.61521059e-01,  2.54889935e-01,\n",
      "       -5.45323968e-01,  1.95869601e+00,  1.09166026e+00,  2.72373199e-01,\n",
      "        1.66386992e-01, -1.62603402e+00,  8.57925892e-01,  8.55967879e-01,\n",
      "       -3.08078051e-01, -2.82261044e-01, -2.71525908e+00, -1.77975500e+00,\n",
      "        1.71776414e+00,  4.94645983e-01, -2.55498895e-03, -1.40434396e+00,\n",
      "       -3.32278907e-01, -1.66640967e-01,  4.50035006e-01, -7.25718915e-01,\n",
      "       -1.11774957e+00,  9.09800053e-01, -8.23984027e-01, -1.11558199e-01,\n",
      "       -2.50831676e+00, -1.00128496e+00, -6.19937122e-01,  6.74065769e-01,\n",
      "        2.37106490e+00, -1.28021407e+00,  8.03367794e-01, -8.72385979e-01,\n",
      "       -1.30116808e+00,  6.90891072e-02, -1.09741282e+00,  3.83603960e-01,\n",
      "        1.41390121e+00, -1.43318927e+00,  1.67475522e-01, -3.26270014e-01,\n",
      "        1.77134991e+00,  5.10958016e-01,  1.81626296e+00,  5.72080970e-01,\n",
      "        1.29188001e+00, -6.34309947e-01,  3.02255893e+00,  1.61584210e+00,\n",
      "        2.98895895e-01,  1.49053991e+00, -8.05352032e-01,  3.18372905e-01,\n",
      "       -9.58126068e-01, -8.80548656e-01, -2.80903339e-01,  5.38213789e-01,\n",
      "       -1.47162545e+00,  1.25667691e+00,  1.45401418e+00, -6.44132078e-01,\n",
      "        6.34513021e-01, -6.52859747e-01,  1.39196503e+00,  1.58480164e-02,\n",
      "       -5.15710115e-02, -1.28081703e+00, -1.01983392e+00,  2.02181792e+00,\n",
      "       -7.90929973e-01, -1.19961607e+00,  1.60096908e+00, -1.15242100e+00,\n",
      "        3.38182402e+00, -4.47189003e-01, -3.56000853e+00, -1.31952202e+00,\n",
      "        2.38508415e+00, -4.67358023e-01, -1.04929984e-01,  1.17190480e+00,\n",
      "       -1.44000506e+00, -2.85600996e+00,  1.64825988e+00, -1.27160001e+00,\n",
      "       -2.84438300e+00, -8.14199567e-01, -1.17913592e+00,  2.11768061e-01,\n",
      "       -2.04390004e-01,  1.02379000e+00, -1.18774295e+00,  4.04473960e-01,\n",
      "        1.71051693e+00,  2.10729033e-01, -1.04176044e-01,  2.64532685e+00,\n",
      "        5.52602649e-01, -1.00034952e-01, -8.32730472e-01,  8.68549868e-02,\n",
      "        2.10704684e+00, -9.12829995e-01, -2.17797711e-01,  4.25815761e-01,\n",
      "        4.89389986e-01, -2.35440874e+00,  1.03726602e+00,  2.99628019e-01,\n",
      "       -2.29518175e+00, -1.19088995e+00, -1.56775594e+00,  4.81357872e-01,\n",
      "        2.58886009e-01,  1.63086486e+00,  1.09298694e+00,  1.51955092e+00,\n",
      "        1.57921600e+00, -3.31018984e-01, -1.32175696e+00,  2.51394004e-01,\n",
      "       -2.66831070e-01, -8.79552841e-01, -1.67054045e+00, -5.98355591e-01,\n",
      "       -1.72442603e+00,  3.53878081e-01, -1.00245789e-01,  8.18583518e-02,\n",
      "       -1.18247008e+00, -2.16274947e-01, -1.53585279e+00, -1.00259495e+00,\n",
      "        3.76058996e-01, -1.08472502e+00,  4.95786011e-01,  5.59544921e-01,\n",
      "       -4.01794046e-01, -2.35278583e+00,  7.50305057e-01, -5.57792008e-01,\n",
      "       -2.67632484e+00, -5.47100008e-01, -1.38525939e+00,  6.01166546e-01,\n",
      "       -3.88157994e-01, -8.74825954e-01, -2.29462862e-01, -1.62397194e+00,\n",
      "        2.03481197e+00,  1.12354732e+00, -3.32110548e+00,  5.65715015e-01,\n",
      "        8.86570215e-02,  3.98891449e-01,  4.01370049e-01,  1.44220090e+00,\n",
      "       -1.51469207e+00, -4.40054417e-01,  4.36042011e-01,  7.98258960e-01,\n",
      "        4.29714918e-01, -1.23478103e+00, -1.33746600e+00, -1.39411166e-01,\n",
      "       -1.73376715e+00,  1.03705084e+00, -1.00783789e+00,  7.64809966e-01,\n",
      "       -2.10387588e+00, -1.20498943e+00,  7.15719998e-01,  2.31029105e+00,\n",
      "        3.13686419e+00,  6.55933976e-01,  1.44422758e+00, -3.58600497e+00,\n",
      "        3.93547058e-01,  1.29717898e+00, -6.41901270e-02,  1.41662729e+00,\n",
      "       -1.55888200e+00,  9.99567986e-01, -4.62384403e-01,  1.49782610e+00,\n",
      "        1.05360794e+00, -1.42077219e+00,  1.45016801e+00, -1.67160600e-01,\n",
      "       -1.64311409e+00,  1.37400889e+00, -1.61007154e+00, -3.38432014e-01,\n",
      "       -1.39598393e+00,  4.63313967e-01, -3.32547009e-01, -2.70135307e+00,\n",
      "       -4.39517164e+00, -2.17735603e-01, -1.59886569e-01, -1.84137464e+00,\n",
      "        1.88658106e+00,  2.78622675e+00, -2.56485999e-01, -1.12930894e+00,\n",
      "        2.35905099e+00,  1.74482191e+00,  2.95478964e+00,  9.66508508e-01,\n",
      "        1.79004666e-04, -1.91343057e+00, -1.64937305e+00, -4.40949686e-02,\n",
      "       -2.20238590e+00,  1.06359792e+00,  9.85469937e-01,  2.55934983e-01,\n",
      "        8.47428024e-01, -9.60515022e-01,  3.25432003e-01, -9.48892951e-01,\n",
      "        6.55815005e-01, -1.07967997e+00, -5.85403979e-01,  1.96668780e+00,\n",
      "        2.56544977e-01,  4.98031080e-01, -3.12920988e-01,  5.87350011e-01,\n",
      "        1.50819802e+00, -1.20058095e+00, -8.11637998e-01, -6.13517880e-01,\n",
      "        6.67746007e-01,  2.90144056e-01, -9.97976303e-01,  1.22212696e+00,\n",
      "        1.94402719e+00,  7.89863110e-01, -6.13927960e-01,  2.79358816e+00,\n",
      "       -1.98756039e-01,  2.93139927e-02, -2.79761863e+00,  2.48111963e-01],\n",
      "      dtype=float32)"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The quick brown fox jumped over the lazy Shiba.')\n",
    "\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6e0077-94bd-4946-888e-e46d63c190db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: (300,)"
     ]
    }
   ],
   "source": [
    "doc.vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74b75231-424b-491a-8351-db688e6ebd5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Identifying similar vectors\n",
    "The best way to expose vector relationships is through the `.similarity()` method of Doc tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f2d3d7-d68a-46c3-96a2-867c09f559e4",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.3854507803916931\n",
      "lion pet 0.20031583309173584\n",
      "cat lion 0.3854507803916931\n",
      "cat cat 1.0\n",
      "cat pet 0.7329663634300232\n",
      "pet lion 0.20031583309173584\n",
      "pet cat 0.7329663634300232\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a three-token Doc object:\n",
    "tokens = nlp(u'lion cat pet')\n",
    "\n",
    "# Iterate through token combinations:\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b1932f-9b3b-4c50-b756-4bd3c30bd9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: -0.03005962407303179"
     ]
    }
   ],
   "source": [
    "nlp(u'Shiba').similarity(nlp(u'floral dress'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bbc2a23-552c-4d11-9630-1936bc378b6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<font color=green>Note that order doesn't matter. `token1.similarity(token2)` has the same value as `token2.similarity(token1)`.</font>\n",
    "#### To view this as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676cea24-4904-4479-8a0b-3e2e27ab8fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table><tr><th></th><th>lion</th><th>cat</th><th>pet</th></tr><tr><td>**lion**</td><td>1.0</td><td>0.3855</td><td>0.2003</td></tr><tr><td>**cat**</td><td>0.3855</td><td>1.0</td><td>0.733</td></tr><tr><td>**pet**</td><td>0.2003</td><td>0.733</td><td>1.0</td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "text/markdown": "<table><tr><th></th><th>lion</th><th>cat</th><th>pet</th></tr><tr><td>**lion**</td><td>1.0</td><td>0.3855</td><td>0.2003</td></tr><tr><td>**cat**</td><td>0.3855</td><td>1.0</td><td>0.733</td></tr><tr><td>**pet**</td><td>0.2003</td><td>0.733</td><td>1.0</td></tr>",
        "text/plain": "<IPython.core.display.Markdown object>"
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "d5c4eecb-00a494c5a7d4e9bacf9a5d0e"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For brevity, assign each token a name\n",
    "a,b,c = tokens\n",
    "\n",
    "# Display as a Markdown table (this only works in Jupyter!)\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f'<table><tr><th></th><th>{a.text}</th><th>{b.text}</th><th>{c.text}</th></tr>\\\n",
    "<tr><td>**{a.text}**</td><td>{a.similarity(a):{.4}}</td><td>{b.similarity(a):{.4}}</td><td>{c.similarity(a):{.4}}</td></tr>\\\n",
    "<tr><td>**{b.text}**</td><td>{a.similarity(b):{.4}}</td><td>{b.similarity(b):{.4}}</td><td>{c.similarity(b):{.4}}</td></tr>\\\n",
    "<tr><td>**{c.text}**</td><td>{a.similarity(c):{.4}}</td><td>{b.similarity(c):{.4}}</td><td>{c.similarity(c):{.4}}</td></tr>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e491c16-ae88-4ea1-9b48-749558c86342",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As expected, we see the strongest similarity between \"cat\" and \"pet\", the weakest between \"lion\" and \"pet\", and some similarity between \"lion\" and \"cat\". A word will have a perfect (1.0) similarity with itself.\n",
    "\n",
    "If you're curious, the similarity between \"lion\" and \"dandelion\" is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f38184-bd1f-4e18-b983-69a18a8d4e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: 0.4145525455932417"
     ]
    }
   ],
   "source": [
    "nlp(u'lion').similarity(nlp(u'dandelion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6b0bbc2-4104-4471-9f2b-0a75fc1758d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Opposites are not necessarily different\n",
    "Words that have opposite meaning, but that often appear in the same *context* may have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f73527-6a01-427a-bdec-6ad0e4588f5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like like 1.0\n",
      "like love 0.5212638974189758\n",
      "like hate 0.5065140724182129\n",
      "love like 0.5212638974189758\n",
      "love love 1.0\n",
      "love hate 0.5708349943161011\n",
      "hate like 0.5065140724182129\n",
      "hate love 0.5708349943161011\n",
      "hate hate 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a three-token Doc object:\n",
    "tokens = nlp(u'like love hate')\n",
    "\n",
    "# Iterate through token combinations:\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52105245-7b6c-4952-b51d-15d4a4fd78f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Vector norms\n",
    "It's sometimes helpful to aggregate 300 dimensions into a [Euclidian (L2) norm](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Euclidean_norm), computed as the square root of the sum-of-squared-vectors. This is accessible as the `.vector_norm` token attribute. Other helpful attributes include `.has_vector` and `.is_oov` or *out of vocabulary*.\n",
    "\n",
    "For example, our 685k vector library may not have the word \"[nargle](https://en.wikibooks.org/wiki/Muggles%27_Guide_to_Harry_Potter/Magic/Nargle)\". To test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f8d1268-0013-4c27-91a5-f1c4e4b882d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "nargle False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat nargle')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abe24ed-adef-454f-9062-4032e5139c78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: (514157, 300)"
     ]
    }
   ],
   "source": [
    "nlp.vocab.vectors.shape  # number of vocabulary words represented by vectors with 300 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0217a7ed-696c-4532-87ba-37a929164061",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Indeed we see that \"nargle\" does not have a vector, so the vector_norm value is zero, and it identifies as *out of vocabulary*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86e6545f-d5b4-4211-85ed-3ccb1e1aae3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Vector arithmetic\n",
    "Believe it or not, we can actually calculate new vectors by adding & subtracting related vectors. A famous example suggests\n",
    "<pre>\"king\" - \"man\" + \"woman\" = \"queen\"</pre>\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3f31e4-6657-4827-ae82-7c7958220f04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'queen', 'the', 'and', 'that', 'women', 'where', 'she', 'they', 'woman']\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)  # cosine distance is complement of cosine similarity\n",
    "\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "# Now we find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "new_vector = king - man + woman\n",
    "computed_similarities = []\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])  # descending order -, by similarity\n",
    "\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d612faa6-1a49-4995-8b2d-f7fd3b7d6804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: [(<spacy.lexeme.Lexeme at 0x7ff37a309340>, 0.8489541411399841),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555800>, 0.6178014278411865),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309d80>, 0.39655405282974243),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a3613c0>, 0.3899005055427551),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e140>, 0.38483577966690063),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309f40>, 0.3667755722999573),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a745140>, 0.3385923206806183),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555200>, 0.32445624470710754),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555e40>, 0.3206636309623718),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555340>, 0.30994713306427),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555480>, 0.30542072653770447),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a3616c0>, 0.29837310314178467),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a4945c0>, 0.29441288113594055),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309880>, 0.2928425371646881),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555900>, 0.2926899194717407),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309ac0>, 0.2898596227169037),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a551d40>, 0.2833782136440277),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309fc0>, 0.283033549785614),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a551e00>, 0.26955559849739075),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ea00>, 0.26932212710380554),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555ec0>, 0.266380250453949),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555180>, 0.25798389315605164),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37acfff80>, 0.2545508146286011),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555600>, 0.2465803474187851),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309480>, 0.2416202425956726),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a5559c0>, 0.24003535509109497),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e200>, 0.23080326616764069),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555bc0>, 0.2254330962896347),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e480>, 0.22187210619449615),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555e80>, 0.21815316379070282),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a3096c0>, 0.2153894305229187),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a745fc0>, 0.21393075585365295),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a3612c0>, 0.2121567577123642),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e3c0>, 0.2090519368648529),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555a40>, 0.20415686070919037),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a494840>, 0.20324556529521942),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a5552c0>, 0.20185594260692596),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555880>, 0.20116958022117615),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a454440>, 0.19990698993206024),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309d00>, 0.19644013047218323),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309680>, 0.19452732801437378),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555f00>, 0.1922803521156311),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309c00>, 0.18580730259418488),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e0c0>, 0.181433767080307),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555240>, 0.1793898493051529),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a551740>, 0.17656052112579346),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555d80>, 0.17582198977470398),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a361680>, 0.17220371961593628),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39eb40>, 0.16010816395282745),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555d00>, 0.15490320324897766),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e340>, 0.13670341670513153),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309780>, 0.13091273605823517),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e840>, 0.12557661533355713),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a533740>, 0.12013807147741318),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a5555c0>, 0.10698670893907547),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a551100>, 0.10431993752717972),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a551340>, 0.10000677406787872),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39eb80>, 0.09778809547424316),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ea40>, 0.09565512090921402),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e680>, 0.08244204521179199),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a5554c0>, 0.07339465618133545),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e980>, 0.07244130969047546),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309240>, 0.0719887912273407),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e2c0>, 0.07003621011972427),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e1c0>, 0.06565330177545547),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e500>, 0.06257739663124084),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309b00>, 0.0622316375374794),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555b80>, 0.061639439314603806),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ed00>, 0.061372771859169006),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e880>, 0.05999797582626343),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff3863ca140>, 0.05524952709674835),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39eec0>, 0.054689355194568634),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309700>, 0.05038555711507797),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309800>, 0.03642822802066803),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e7c0>, 0.03469546511769295),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ea80>, 0.03319096937775612),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555500>, 0.03249751776456833),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e940>, 0.029356814920902252),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555040>, 0.02749769762158394),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309ec0>, 0.02613031677901745),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e5c0>, 0.0261156614869833),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e400>, 0.022534290328621864),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309140>, 0.021381396800279617),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a745400>, 0.017747921869158745),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309e00>, 0.017566954717040062),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309440>, 0.016177993267774582),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e280>, 0.01613551378250122),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ecc0>, 0.014911837875843048),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555c80>, 0.01251276396214962),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a551f80>, 0.008380548097193241),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309c40>, 0.0066097755916416645),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a3091c0>, 0.005720385815948248),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e040>, 0.004759781062602997),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a494fc0>, 0.004386077634990215),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ef40>, 0.0009621680364944041),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309740>, -0.001242928672581911),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555080>, -0.001569278072565794),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a5553c0>, -0.0031983337830752134),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ee00>, -0.0035099152009934187),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ef00>, -0.006476094946265221),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a551500>, -0.007181430701166391),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e6c0>, -0.007245962042361498),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309600>, -0.010682614520192146),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555e00>, -0.01189793273806572),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a361840>, -0.013968191109597683),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a361780>, -0.0174685250967741),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555b40>, -0.022200554609298706),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a551ac0>, -0.03126813471317291),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e700>, -0.04106904938817024),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e8c0>, -0.04508766159415245),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555300>, -0.04832000285387039),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e180>, -0.05902189388871193),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309dc0>, -0.06617677956819534),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e580>, -0.06810466200113297),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a5550c0>, -0.07287076115608215),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a5557c0>, -0.07636149227619171),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555400>, -0.07872253656387329),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309f80>, -0.08220101147890091),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a361480>, -0.08385646343231201),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a361700>, -0.08980458229780197),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ed80>, -0.0951840877532959),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a5556c0>, -0.10074420273303986),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309580>, -0.1028217151761055),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555700>, -0.12722618877887726),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39ee40>, -0.1318208873271942),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555f80>, -0.14137345552444458),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a39e600>, -0.14783941209316254),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555980>, -0.20390625298023224),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a361200>, -0.20889312028884888),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a309080>, -0.21016256511211395),\n",
      " (<spacy.lexeme.Lexeme at 0x7ff37a555ac0>, -0.2223987877368927)]"
     ]
    }
   ],
   "source": [
    "computed_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d735b2bb-feca-4003-bb5e-b82580dc30c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "So in this case, \"king\" was still closer than \"queen\" to our calculated vector, although \"queen\" did show up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3c0df38-2191-4c67-ae21-eed5716ad5bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next up: Sentiment Analysis"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-Semantics-and-Word-Vectors",
   "notebookOrigID": 3520988577289764,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
